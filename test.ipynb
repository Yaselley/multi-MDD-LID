{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dict_vocab import *\n",
    "from tokenizer_extractor import *\n",
    "from datacollator import *\n",
    "from mvModel import *\n",
    "from mvTrainer import *\n",
    "from result import *\n",
    "import argparse\n",
    "import os\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoConfig, AutoModel, Wav2Vec2Config, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ɛ': 0,\n",
       " 'ŋ': 1,\n",
       " 'v': 2,\n",
       " 'l': 3,\n",
       " 'iː': 4,\n",
       " 'g': 5,\n",
       " 'ʌ': 6,\n",
       " 'b': 7,\n",
       " 'f': 8,\n",
       " 'ʒ': 9,\n",
       " 'θ': 10,\n",
       " 'uː': 11,\n",
       " 'z': 12,\n",
       " 'eɪ': 13,\n",
       " 'n': 14,\n",
       " 'p': 15,\n",
       " 'm': 16,\n",
       " 'ᵻ': 17,\n",
       " 'ɜː': 18,\n",
       " 'h': 19,\n",
       " 'ð': 20,\n",
       " 'd': 21,\n",
       " 'ɐ': 22,\n",
       " 't': 23,\n",
       " 'k': 24,\n",
       " 's': 25,\n",
       " 'ɪ': 26,\n",
       " 'ɹ': 27,\n",
       " 'aɪ': 28,\n",
       " 'w': 29,\n",
       " '|': 30,\n",
       " '[UNK]': 31,\n",
       " '[PAD]': 32}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_vocab = VocabDict(root_dir ='/alt-asr/yelkheir/wav2vec_arabic/test_data', out_dir ='/alt-asr/yelkheir/wav2vec_arabic/test_data', label='ref_anno')\n",
    "create_vocab.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = '/alt-asr/yelkheir/wav2vec_arabic/test_data'+'/vocab.json'\n",
    "model_name = 'facebook/wav2vec2-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_extractor = TokenizerExtractor(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/export/home/yelkheir/.cache/huggingface/datasets/csv/default-aea5198dbececd98/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005587577819824219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c683f8a0ff74a589080c7ac36868c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": '/alt-asr/yelkheir/wav2vec_arabic/test_data'+\"/train.csv\",\n",
    "    \"eval\": '/alt-asr/yelkheir/wav2vec_arabic/test_data'+\"/eval.csv\",\n",
    "    \"test\": '/alt-asr/yelkheir/wav2vec_arabic/test_data'+\"/test.csv\"}\n",
    "\n",
    "dataset = load_dataset('csv',data_files=data_files, delimiter=\",\",cache_dir=\"\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"eval\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009152889251708984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map (num_proc=4)",
       "rate": null,
       "total": 4,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dd261431f2483795efc3dd4d7096d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ð', '|', 'ɛ', '|', 'ɛ', '|', 't', '|', 'd', '|', 'ɪ', '|', 'n', '|', 'ᵻ', '|', 'ɹ', '|', 'ɐ', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ɪ', '|', 'f', '|', 'θ', '|', 'ɐ', '|', 'n', '|', 'ɹ', '|', 'ɛ', '|', 's', '|', 't', '|', 'ᵻ', '|', 'd', '|', 'f', '|', 'ʌ', '|', 'ɹ', '|', 't', '|', 'uː', '|', 'ʌ', '|', 'ɜː', '|', 's']['ɛ', '|', 'v', '|', 'ɹ', '|', 'ɪ', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 'w', '|', 'ʌ', '|', 'z', '|', 'w', '|', 'ɜː', '|', 'k', '|', 'ɪ', '|', 'ŋ', '|', 'g', '|', 's', '|', 'm', '|', 'uː', '|', 't', '|', 'l', '|', 'iː', '|', 'b', '|', 'ɛ', '|', 't', '|', 'ɹ', '|', 'ð', '|', 'ɐ', '|', 'n', '|', 'aɪ', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ɪ', '|', 'k', '|', 's', '|', 'p', '|', 'ɛ', '|', 'k', '|', 't', '|', 'ᵻ', '|', 'd']"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ð', '|', 'ᵻ', '|', 's', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ð', '|', 'ᵻ', '|', 'ɹ', '|', 'ʌ', '|', 'w', '|', 'ɪ', '|', 'l', '|', 'ᵻ', '|', 'n', '|', 'ᵻ', '|', 's', '|', 'p', '|', 'ɹ', '|', 'iː', '|', 'p', '|', 'ɛ', '|', 'ɹ', '|', 'd', '|', 'h', '|', 'ɪ', '|', 'm', '|', 'f', '|', 'ɹ', '|', 'ɜː', '|', 'd', '|', 'ð', '|', 'ɪ', '|', 's', '|', 'd', '|', 'eɪ']['ð', '|', 'eɪ', '|', 'eɪ', '|', 't', '|', 'd', '|', 'ɪ', '|', 'n', '|', 'ɜː', '|', 'ɐ', '|', 't', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ɪ', '|', 'f', '|', 'θ', '|', 'ɐ', '|', 'n', '|', 'd', '|', 'ɹ', '|', 'ɛ', '|', 's', '|', 't', '|', 'ᵻ', '|', 'd', '|', 'f', '|', 'ʌ', '|', 'ɹ', '|', 't', '|', 'uː', '|', 'a', '|', 'ʊ', '|', 'ɜː', '|', 'z']['ɛ', '|', 'v', '|', 'ɹ', '|', 'iː', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 'w', '|', 'ʌ', '|', 'z', '|', 'w', '|', 'ɜː', '|', 'k', '|', 'ɪ', '|', 'ŋ', '|', 's', '|', 'm', '|', 'uː', '|', 'ð', '|', 'l', '|', 'iː', '|', 'b', '|', 'ɛ', '|', 't', '|', 'ɜː', '|', 'ð', '|', 'ɐ', '|', 'n', '|', 'aɪ', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ɪ', '|', 'k', '|', 's', '|', 'p', '|', 'ɛ', '|', 'k', '|', 't', '|', 'ᵻ', '|', 'd']\n",
      "['ᵻ', '|', 'b', '|', 'ɪ', '|', 'k', '|', 'k', '|', 'ɐ', '|', 'n', '|', 'v', '|', 'ᵻ', '|', 's', '|', 't', '|', 'ɛ', '|', 'n', '|', 't', '|', 'w', '|', 'ᵻ', '|', 'z', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ᵻ', '|', 's', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 't', '|', 'uː', '|', 'k', '|', 'ᵻ', '|', 'm', '|', 'w', '|', 'ɪ', '|', 'θ', '|', 'ɪ', '|', 'n', '|', 'h', '|', 'ɪ', '|', 's', '|', 'v', '|', 'ɪ', '|', 'ʒ', '|', 'ᵻ', '|', 'n']\n",
      "\n",
      "['ð', '|', 'ᵻ', '|', 's', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ð', '|', 'ᵻ', '|', 'ɹ', '|', 'ʌ', '|', 'w', '|', 'ɪ', '|', 'l', '|', 'd', '|', 'ɜː', '|', 'n', '|', 'ᵻ', '|', 's', '|', 'p', '|', 'ɹ', '|', 'iː', '|', 'p', '|', 'ɛ', '|', 'ɹ', '|', 'd', '|', 'h', '|', 'ɪ', '|', 'm', '|', 'f', '|', 'ɹ', '|', 'ɜː', '|', 'ð', '|', 'ɪ', '|', 's', '|', 'd', '|', 'eɪ']\n",
      "\n",
      "['ᵻ', '|', 'b', '|', 'ɪ', '|', 'g', '|', 'k', '|', 'ɐ', '|', 'n', '|', 'v', '|', 'ᵻ', '|', 's', '|', 't', '|', 'ɛ', '|', 'n', '|', 't', '|', 'w', '|', 'ᵻ', '|', 'z', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ɜː', '|', 's', '|', 't', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 't', '|', 'uː', '|', 'k', '|', 'ᵻ', '|', 'm', '|', 'w', '|', 'ɪ', '|', 'θ', '|', 'ɪ', '|', 'n', '|', 'h', '|', 'ɪ', '|', 'z', '|', 'v', '|', 'ɪ', '|', 'ʒ', '|', 'ᵻ', '|', 'n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008134841918945312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map (num_proc=4)",
       "rate": null,
       "total": 4,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1775e8b53a4b87908142a4dcde1ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ð', '|', 'ɛ', '|', 'ɛ', '|', 't', '|', 'd', '|', 'ɪ', '|', 'n', '|', 'ᵻ', '|', 'ɹ', '|', 'ɐ', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ɪ', '|', 'f', '|', 'θ', '|', 'ɐ', '|', 'n', '|', 'ɹ', '|', 'ɛ', '|', 's', '|', 't', '|', 'ᵻ', '|', 'd', '|', 'f', '|', 'ʌ', '|', 'ɹ', '|', 't', '|', 'uː', '|', 'ʌ', '|', 'ɜː', '|', 's']\n",
      "['ð', '|', 'eɪ', '|', 'eɪ', '|', 't', '|', 'd', '|', 'ɪ', '|', 'n', '|', 'ɜː', '|', 'ɐ', '|', 't', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ɪ', '|', 'f', '|', 'θ', '|', 'ɐ', '|', 'n', '|', 'd', '|', 'ɹ', '|', 'ɛ', '|', 's', '|', 't', '|', 'ᵻ', '|', 'd', '|', 'f', '|', 'ʌ', '|', 'ɹ', '|', 't', '|', 'uː', '|', 'a', '|', 'ʊ', '|', 'ɜː', '|', 'z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ð', '|', 'ᵻ', '|', 's', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ð', '|', 'ᵻ', '|', 'ɹ', '|', 'ʌ', '|', 'w', '|', 'ɪ', '|', 'l', '|', 'ᵻ', '|', 'n', '|', 'ᵻ', '|', 's', '|', 'p', '|', 'ɹ', '|', 'iː', '|', 'p', '|', 'ɛ', '|', 'ɹ', '|', 'd', '|', 'h', '|', 'ɪ', '|', 'm', '|', 'f', '|', 'ɹ', '|', 'ɜː', '|', 'd', '|', 'ð', '|', 'ɪ', '|', 's', '|', 'd', '|', 'eɪ']['ɛ', '|', 'v', '|', 'ɹ', '|', 'ɪ', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 'w', '|', 'ʌ', '|', 'z', '|', 'w', '|', 'ɜː', '|', 'k', '|', 'ɪ', '|', 'ŋ', '|', 'g', '|', 's', '|', 'm', '|', 'uː', '|', 't', '|', 'l', '|', 'iː', '|', 'b', '|', 'ɛ', '|', 't', '|', 'ɹ', '|', 'ð', '|', 'ɐ', '|', 'n', '|', 'aɪ', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ɪ', '|', 'k', '|', 's', '|', 'p', '|', 'ɛ', '|', 'k', '|', 't', '|', 'ᵻ', '|', 'd']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ɛ', '|', 'v', '|', 'ɹ', '|', 'iː', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 'w', '|', 'ʌ', '|', 'z', '|', 'w', '|', 'ɜː', '|', 'k', '|', 'ɪ', '|', 'ŋ', '|', 's', '|', 'm', '|', 'uː', '|', 'ð', '|', 'l', '|', 'iː', '|', 'b', '|', 'ɛ', '|', 't', '|', 'ɜː', '|', 'ð', '|', 'ɐ', '|', 'n', '|', 'aɪ', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ɪ', '|', 'k', '|', 's', '|', 'p', '|', 'ɛ', '|', 'k', '|', 't', '|', 'ᵻ', '|', 'd']['ð', '|', 'ᵻ', '|', 's', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ð', '|', 'ᵻ', '|', 'ɹ', '|', 'ʌ', '|', 'w', '|', 'ɪ', '|', 'l', '|', 'd', '|', 'ɜː', '|', 'n', '|', 'ᵻ', '|', 's', '|', 'p', '|', 'ɹ', '|', 'iː', '|', 'p', '|', 'ɛ', '|', 'ɹ', '|', 'd', '|', 'h', '|', 'ɪ', '|', 'm', '|', 'f', '|', 'ɹ', '|', 'ɜː', '|', 'ð', '|', 'ɪ', '|', 's', '|', 'd', '|', 'eɪ']['ᵻ', '|', 'b', '|', 'ɪ', '|', 'k', '|', 'k', '|', 'ɐ', '|', 'n', '|', 'v', '|', 'ᵻ', '|', 's', '|', 't', '|', 'ɛ', '|', 'n', '|', 't', '|', 'w', '|', 'ᵻ', '|', 'z', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ᵻ', '|', 's', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 't', '|', 'uː', '|', 'k', '|', 'ᵻ', '|', 'm', '|', 'w', '|', 'ɪ', '|', 'θ', '|', 'ɪ', '|', 'n', '|', 'h', '|', 'ɪ', '|', 's', '|', 'v', '|', 'ɪ', '|', 'ʒ', '|', 'ᵻ', '|', 'n']\n",
      "\n",
      "\n",
      "['ᵻ', '|', 'b', '|', 'ɪ', '|', 'g', '|', 'k', '|', 'ɐ', '|', 'n', '|', 'v', '|', 'ᵻ', '|', 's', '|', 't', '|', 'ɛ', '|', 'n', '|', 't', '|', 'w', '|', 'ᵻ', '|', 'z', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ɜː', '|', 's', '|', 't', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 't', '|', 'uː', '|', 'k', '|', 'ᵻ', '|', 'm', '|', 'w', '|', 'ɪ', '|', 'θ', '|', 'ɪ', '|', 'n', '|', 'h', '|', 'ɪ', '|', 'z', '|', 'v', '|', 'ɪ', '|', 'ʒ', '|', 'ᵻ', '|', 'n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008858919143676758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map (num_proc=4)",
       "rate": null,
       "total": 4,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efe63a946f241bc93cb3e8727783c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ð', '|', 'ᵻ', '|', 's', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ð', '|', 'ᵻ', '|', 'ɹ', '|', 'ʌ', '|', 'w', '|', 'ɪ', '|', 'l', '|', 'ᵻ', '|', 'n', '|', 'ᵻ', '|', 's', '|', 'p', '|', 'ɹ', '|', 'iː', '|', 'p', '|', 'ɛ', '|', 'ɹ', '|', 'd', '|', 'h', '|', 'ɪ', '|', 'm', '|', 'f', '|', 'ɹ', '|', 'ɜː', '|', 'd', '|', 'ð', '|', 'ɪ', '|', 's', '|', 'd', '|', 'eɪ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ð', '|', 'ᵻ', '|', 's', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ð', '|', 'ᵻ', '|', 'ɹ', '|', 'ʌ', '|', 'w', '|', 'ɪ', '|', 'l', '|', 'd', '|', 'ɜː', '|', 'n', '|', 'ᵻ', '|', 's', '|', 'p', '|', 'ɹ', '|', 'iː', '|', 'p', '|', 'ɛ', '|', 'ɹ', '|', 'd', '|', 'h', '|', 'ɪ', '|', 'm', '|', 'f', '|', 'ɹ', '|', 'ɜː', '|', 'ð', '|', 'ɪ', '|', 's', '|', 'd', '|', 'eɪ']['ᵻ', '|', 'b', '|', 'ɪ', '|', 'k', '|', 'k', '|', 'ɐ', '|', 'n', '|', 'v', '|', 'ᵻ', '|', 's', '|', 't', '|', 'ɛ', '|', 'n', '|', 't', '|', 'w', '|', 'ᵻ', '|', 'z', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ᵻ', '|', 's', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 't', '|', 'uː', '|', 'k', '|', 'ᵻ', '|', 'm', '|', 'w', '|', 'ɪ', '|', 'θ', '|', 'ɪ', '|', 'n', '|', 'h', '|', 'ɪ', '|', 's', '|', 'v', '|', 'ɪ', '|', 'ʒ', '|', 'ᵻ', '|', 'n']\n",
      "\n",
      "['ᵻ', '|', 'b', '|', 'ɪ', '|', 'g', '|', 'k', '|', 'ɐ', '|', 'n', '|', 'v', '|', 'ᵻ', '|', 's', '|', 't', '|', 'ɛ', '|', 'n', '|', 't', '|', 'w', '|', 'ᵻ', '|', 'z', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ɜː', '|', 's', '|', 't', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 't', '|', 'uː', '|', 'k', '|', 'ᵻ', '|', 'm', '|', 'w', '|', 'ɪ', '|', 'θ', '|', 'ɪ', '|', 'n', '|', 'h', '|', 'ɪ', '|', 'z', '|', 'v', '|', 'ɪ', '|', 'ʒ', '|', 'ᵻ', '|', 'n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ð', '|', 'ɛ', '|', 'ɛ', '|', 't', '|', 'd', '|', 'ɪ', '|', 'n', '|', 'ᵻ', '|', 'ɹ', '|', 'ɐ', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ɪ', '|', 'f', '|', 'θ', '|', 'ɐ', '|', 'n', '|', 'ɹ', '|', 'ɛ', '|', 's', '|', 't', '|', 'ᵻ', '|', 'd', '|', 'f', '|', 'ʌ', '|', 'ɹ', '|', 't', '|', 'uː', '|', 'ʌ', '|', 'ɜː', '|', 's']\n",
      "['ð', '|', 'eɪ', '|', 'eɪ', '|', 't', '|', 'd', '|', 'ɪ', '|', 'n', '|', 'ɜː', '|', 'ɐ', '|', 't', '|', 'ð', '|', 'ᵻ', '|', 'f', '|', 'ɪ', '|', 'f', '|', 'θ', '|', 'ɐ', '|', 'n', '|', 'd', '|', 'ɹ', '|', 'ɛ', '|', 's', '|', 't', '|', 'ᵻ', '|', 'd', '|', 'f', '|', 'ʌ', '|', 'ɹ', '|', 't', '|', 'uː', '|', 'a', '|', 'ʊ', '|', 'ɜː', '|', 'z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ɛ', '|', 'v', '|', 'ɹ', '|', 'ɪ', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 'w', '|', 'ʌ', '|', 'z', '|', 'w', '|', 'ɜː', '|', 'k', '|', 'ɪ', '|', 'ŋ', '|', 'g', '|', 's', '|', 'm', '|', 'uː', '|', 't', '|', 'l', '|', 'iː', '|', 'b', '|', 'ɛ', '|', 't', '|', 'ɹ', '|', 'ð', '|', 'ɐ', '|', 'n', '|', 'aɪ', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ɪ', '|', 'k', '|', 's', '|', 'p', '|', 'ɛ', '|', 'k', '|', 't', '|', 'ᵻ', '|', 'd']\n",
      "['ɛ', '|', 'v', '|', 'ɹ', '|', 'iː', '|', 'θ', '|', 'ɪ', '|', 'ŋ', '|', 'w', '|', 'ʌ', '|', 'z', '|', 'w', '|', 'ɜː', '|', 'k', '|', 'ɪ', '|', 'ŋ', '|', 's', '|', 'm', '|', 'uː', '|', 'ð', '|', 'l', '|', 'iː', '|', 'b', '|', 'ɛ', '|', 't', '|', 'ɜː', '|', 'ð', '|', 'ɐ', '|', 'n', '|', 'aɪ', '|', 'h', '|', 'ɐ', '|', 'd', '|', 'ɪ', '|', 'k', '|', 's', '|', 'p', '|', 'ɛ', '|', 'k', '|', 't', '|', 'ᵻ', '|', 'd']\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "def speech_file_to_array_fn(path):\n",
    "    speech_array, sampling_rate = librosa.load(path, sr=16_000)\n",
    "    return speech_array\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"path\"]\n",
    "    array = speech_file_to_array_fn(audio)\n",
    "    batch[\"input_values\"] = tokenizer_extractor.processor(array, sampling_rate=16000).input_values[0]\n",
    "    with tokenizer_extractor.processor.as_target_processor():\n",
    "        batch[\"labels_p\"] = tokenizer_extractor.processor(batch['ref_anno']).input_ids\n",
    "        batch[\"labels_p_ref\"] = tokenizer_extractor.processor(batch['ref_ref']).input_ids\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_dataset, num_proc = 128)\n",
    "eval_dataset = eval_dataset.map(prepare_dataset, num_proc = 128)\n",
    "test_dataset = test_dataset.map(prepare_dataset, num_proc = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = MVDataCollatorBatchWisePadding(processor=tokenizer_extractor.processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3692"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from mvModel import *\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(tokenizer_extractor.tokenizer),\n",
    "    finetuning_task=\"name_exp\"\n",
    ")\n",
    "classes = 768\n",
    "setattr(config, 'model_name_audio', model_name)\n",
    "setattr(config, 'vocab_size', len(tokenizer_extractor.tokenizer))\n",
    "setattr(config, 'classifier_proj_size', len(tokenizer_extractor.tokenizer))\n",
    "setattr(config, 'pad_token_id', tokenizer_extractor.tokenizer.pad_token_id)\n",
    "\n",
    "model = SSLforJointClassification(config=config)\n",
    "model.freeze_feature_extractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/alt-asr/yelkheir/wav2vec_arabic/test_data',\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    max_steps=2,\n",
    "    save_steps=1000,  #100,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=2000,\n",
    "    save_total_limit=500,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainerJoint(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer_extractor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'inputs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/alt-asr/yelkheir/wav2vec_arabic/utils_mms_text/mvTrainer.py:19\u001b[0m, in \u001b[0;36mModelTrainerBase.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     18\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_inputs(inputs)\n\u001b[0;32m---> 19\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     21\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/alt-asr/yelkheir/wav2vec_arabic/utils_mms_text/mvTrainer.py:75\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mModelTrainerJoint\u001b[39;00m(ModelTrainerBase):\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_loss\u001b[39m(\u001b[39mself\u001b[39m, model, inputs, return_outputs\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     77\u001b[0m         loss, logits \u001b[39m=\u001b[39m get_outputs(model, inputs)\n\u001b[1;32m     78\u001b[0m         \u001b[39mreturn\u001b[39;00m (loss, outputs) \u001b[39mif\u001b[39;00m return_outputs \u001b[39melse\u001b[39;00m loss\n",
      "File \u001b[0;32m/alt-asr/yelkheir/wav2vec_arabic/utils_mms_text/mvTrainer.py:53\u001b[0m, in \u001b[0;36mget_outputs\u001b[0;34m(model, inputs, eval)\u001b[0m\n\u001b[1;32m     51\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     52\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 53\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m/home/local/QCRI/yelkheir/anaconda3/envs/ssl/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/alt-asr/yelkheir/wav2vec_arabic/utils_mms_text/mvModel.py:46\u001b[0m, in \u001b[0;36mSSLforJointClassification.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     35\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     36\u001b[0m     input_values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     42\u001b[0m ):\n\u001b[1;32m     44\u001b[0m     return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m---> 46\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretrained\u001b[39m.\u001b[39mwav2vec2(\n\u001b[1;32m     49\u001b[0m         input_values,\n\u001b[1;32m     50\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m         return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     56\u001b[0m     hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'inputs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ssl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "737c0b2d2c3c26a5cd520d9ac05a519189fa873264e48e00698171e1270a15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
